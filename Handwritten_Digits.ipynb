{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrDlTL+xPwZwQhhulIFgPs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsilryan/Algorithms/blob/master/Handwritten_Digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9b0qgQi5HBva"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vmN6eTWxHTxO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor(),\n",
        "    download = True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoBgZuToOpUF",
        "outputId": "ceee4694-908a-431e-f062-ea5d9c84420a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:02<00:00, 4594060.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 133864.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:02<00:00, 635071.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4250452.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOc_fD53PDCS",
        "outputId": "9723c161-efbe-48bc-eb4f-5cf68f78aa23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.data.shape # .size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7prFj2_PWj2",
        "outputId": "e3af2a9a-06be-4773-93e7-00d99c3862f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O63pKtNIPvXF",
        "outputId": "74a1f101-61da-474e-ace5-2b19c860c13b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loaders = {\n",
        "    'train' : DataLoader (\n",
        "        train_data,\n",
        "        batch_size = 100,\n",
        "        shuffle = True,\n",
        "        num_workers = 1\n",
        "    ),\n",
        "\n",
        "    'test' : DataLoader (\n",
        "        test_data,\n",
        "        batch_size = 100,\n",
        "        shuffle = True,\n",
        "        num_workers = 1\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "e4ObGopiSLOl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL57RLF7SjAB",
        "outputId": "d0eec34d-b168-4e62-c54b-84a95d370310"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': <torch.utils.data.dataloader.DataLoader at 0x7c1e93ee5540>,\n",
              " 'test': <torch.utils.data.dataloader.DataLoader at 0x7c1e93ee44f0>}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.conv2_dropout = nn.Dropout2d() # Regularlization layer - Based on probability it randomly deactivates certain nodes in the network\n",
        "    # Fully connected layer -> Linear\n",
        "    self.fc1 = nn.Linear(320, 50) # 320 -> 20 (Hidden Layers from Conv2) * 4 * 4 (4 are convolutions)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_dropout(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320) # Reshapes the data for linear layer -> Flattens\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training = self.training)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return F.softmax(x) # Returns probability of each number\n"
      ],
      "metadata": {
        "id": "GJd1MowWTrh3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    params = model.parameters(),\n",
        "    lr = 0.001\n",
        "  )\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "IPCmnDkkHFG2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # 1. Forward Pass\n",
        "    prediction = model(data)\n",
        "\n",
        "    # 2. Calculate the Loss\n",
        "    loss = loss_fn(prediction, target)\n",
        "\n",
        "    # 3. Zero_Grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer Step\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 20 == 0:\n",
        "      print(f\"Train Epoch: {epoch} [{batch_idx * len(data)} / {len(loaders['train'].dataset)} ({100. * batch_idx / len(loaders['train']) :.0f}%)]\\tLoss: {loss.item() :.6f}\")\n",
        "\n",
        "def test():\n",
        "  model.eval()\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for data, target in loaders['test']:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # 1. Forward Pass\n",
        "      output = model(data)\n",
        "\n",
        "      # 2. Calculate the loss\n",
        "      test_loss += loss_fn(output, target).item()\n",
        "\n",
        "      # 3. Accuracy\n",
        "      test_pred = output.argmax(dim = 1, keepdim = True)\n",
        "      correct += test_pred.eq(target.view_as(test_pred)).sum().item()\n",
        "\n",
        "  test_loss /= len(loaders['test'].dataset)\n",
        "  print(f\"\\nTest set -> Average loss: {test_loss:.4f} | Accuracy {correct} / {len(loaders['test'].dataset)} ({100. * correct / len(loaders['test'].dataset):.0f})%\\n\")\n"
      ],
      "metadata": {
        "id": "raHspg00H-kT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BuaoYldN6Ni",
        "outputId": "b5eb8fee-2af1-4dbc-fd65-aae434b2c760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-57077a1fc1fa>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x) # Returns probability of each number\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0 / 60000 (0%)]\tLoss: 2.302442\n",
            "Train Epoch: 0 [2000 / 60000 (3%)]\tLoss: 2.281388\n",
            "Train Epoch: 0 [4000 / 60000 (7%)]\tLoss: 2.119477\n",
            "Train Epoch: 0 [6000 / 60000 (10%)]\tLoss: 2.000457\n",
            "Train Epoch: 0 [8000 / 60000 (13%)]\tLoss: 1.887352\n",
            "Train Epoch: 0 [10000 / 60000 (17%)]\tLoss: 1.887102\n",
            "Train Epoch: 0 [12000 / 60000 (20%)]\tLoss: 1.822305\n",
            "Train Epoch: 0 [14000 / 60000 (23%)]\tLoss: 1.755247\n",
            "Train Epoch: 0 [16000 / 60000 (27%)]\tLoss: 1.765471\n",
            "Train Epoch: 0 [18000 / 60000 (30%)]\tLoss: 1.702966\n",
            "Train Epoch: 0 [20000 / 60000 (33%)]\tLoss: 1.786553\n",
            "Train Epoch: 0 [22000 / 60000 (37%)]\tLoss: 1.699498\n",
            "Train Epoch: 0 [24000 / 60000 (40%)]\tLoss: 1.678521\n",
            "Train Epoch: 0 [26000 / 60000 (43%)]\tLoss: 1.679915\n",
            "Train Epoch: 0 [28000 / 60000 (47%)]\tLoss: 1.612811\n",
            "Train Epoch: 0 [30000 / 60000 (50%)]\tLoss: 1.643028\n",
            "Train Epoch: 0 [32000 / 60000 (53%)]\tLoss: 1.625650\n",
            "Train Epoch: 0 [34000 / 60000 (57%)]\tLoss: 1.661282\n",
            "Train Epoch: 0 [36000 / 60000 (60%)]\tLoss: 1.607840\n",
            "Train Epoch: 0 [38000 / 60000 (63%)]\tLoss: 1.634479\n",
            "Train Epoch: 0 [40000 / 60000 (67%)]\tLoss: 1.621350\n",
            "Train Epoch: 0 [42000 / 60000 (70%)]\tLoss: 1.617372\n",
            "Train Epoch: 0 [44000 / 60000 (73%)]\tLoss: 1.650418\n",
            "Train Epoch: 0 [46000 / 60000 (77%)]\tLoss: 1.617848\n",
            "Train Epoch: 0 [48000 / 60000 (80%)]\tLoss: 1.594726\n",
            "Train Epoch: 0 [50000 / 60000 (83%)]\tLoss: 1.613824\n",
            "Train Epoch: 0 [52000 / 60000 (87%)]\tLoss: 1.590581\n",
            "Train Epoch: 0 [54000 / 60000 (90%)]\tLoss: 1.609781\n",
            "Train Epoch: 0 [56000 / 60000 (93%)]\tLoss: 1.564092\n",
            "Train Epoch: 0 [58000 / 60000 (97%)]\tLoss: 1.592551\n",
            "\n",
            "Test set -> Average loss: 0.0152 | Accuracy 9388 / 10000 (94)%\n",
            "\n",
            "Train Epoch: 1 [0 / 60000 (0%)]\tLoss: 1.624164\n",
            "Train Epoch: 1 [2000 / 60000 (3%)]\tLoss: 1.599909\n",
            "Train Epoch: 1 [4000 / 60000 (7%)]\tLoss: 1.598812\n",
            "Train Epoch: 1 [6000 / 60000 (10%)]\tLoss: 1.585646\n",
            "Train Epoch: 1 [8000 / 60000 (13%)]\tLoss: 1.571171\n",
            "Train Epoch: 1 [10000 / 60000 (17%)]\tLoss: 1.556024\n",
            "Train Epoch: 1 [12000 / 60000 (20%)]\tLoss: 1.598922\n",
            "Train Epoch: 1 [14000 / 60000 (23%)]\tLoss: 1.606822\n",
            "Train Epoch: 1 [16000 / 60000 (27%)]\tLoss: 1.664564\n",
            "Train Epoch: 1 [18000 / 60000 (30%)]\tLoss: 1.617384\n",
            "Train Epoch: 1 [20000 / 60000 (33%)]\tLoss: 1.600002\n",
            "Train Epoch: 1 [22000 / 60000 (37%)]\tLoss: 1.551331\n",
            "Train Epoch: 1 [24000 / 60000 (40%)]\tLoss: 1.582139\n",
            "Train Epoch: 1 [26000 / 60000 (43%)]\tLoss: 1.547285\n",
            "Train Epoch: 1 [28000 / 60000 (47%)]\tLoss: 1.550142\n",
            "Train Epoch: 1 [30000 / 60000 (50%)]\tLoss: 1.569252\n",
            "Train Epoch: 1 [32000 / 60000 (53%)]\tLoss: 1.515094\n",
            "Train Epoch: 1 [34000 / 60000 (57%)]\tLoss: 1.594533\n",
            "Train Epoch: 1 [36000 / 60000 (60%)]\tLoss: 1.601398\n",
            "Train Epoch: 1 [38000 / 60000 (63%)]\tLoss: 1.629297\n",
            "Train Epoch: 1 [40000 / 60000 (67%)]\tLoss: 1.600863\n",
            "Train Epoch: 1 [42000 / 60000 (70%)]\tLoss: 1.552219\n",
            "Train Epoch: 1 [44000 / 60000 (73%)]\tLoss: 1.569146\n",
            "Train Epoch: 1 [46000 / 60000 (77%)]\tLoss: 1.577439\n",
            "Train Epoch: 1 [48000 / 60000 (80%)]\tLoss: 1.554338\n",
            "Train Epoch: 1 [50000 / 60000 (83%)]\tLoss: 1.529289\n",
            "Train Epoch: 1 [52000 / 60000 (87%)]\tLoss: 1.617664\n",
            "Train Epoch: 1 [54000 / 60000 (90%)]\tLoss: 1.604380\n",
            "Train Epoch: 1 [56000 / 60000 (93%)]\tLoss: 1.565428\n",
            "Train Epoch: 1 [58000 / 60000 (97%)]\tLoss: 1.582061\n",
            "\n",
            "Test set -> Average loss: 0.0151 | Accuracy 9536 / 10000 (95)%\n",
            "\n",
            "Train Epoch: 2 [0 / 60000 (0%)]\tLoss: 1.530082\n",
            "Train Epoch: 2 [2000 / 60000 (3%)]\tLoss: 1.575072\n",
            "Train Epoch: 2 [4000 / 60000 (7%)]\tLoss: 1.563323\n",
            "Train Epoch: 2 [6000 / 60000 (10%)]\tLoss: 1.580511\n",
            "Train Epoch: 2 [8000 / 60000 (13%)]\tLoss: 1.516595\n",
            "Train Epoch: 2 [10000 / 60000 (17%)]\tLoss: 1.557123\n",
            "Train Epoch: 2 [12000 / 60000 (20%)]\tLoss: 1.551243\n",
            "Train Epoch: 2 [14000 / 60000 (23%)]\tLoss: 1.574534\n",
            "Train Epoch: 2 [16000 / 60000 (27%)]\tLoss: 1.556653\n",
            "Train Epoch: 2 [18000 / 60000 (30%)]\tLoss: 1.507198\n",
            "Train Epoch: 2 [20000 / 60000 (33%)]\tLoss: 1.531657\n",
            "Train Epoch: 2 [22000 / 60000 (37%)]\tLoss: 1.541667\n",
            "Train Epoch: 2 [24000 / 60000 (40%)]\tLoss: 1.570960\n",
            "Train Epoch: 2 [26000 / 60000 (43%)]\tLoss: 1.592550\n",
            "Train Epoch: 2 [28000 / 60000 (47%)]\tLoss: 1.568152\n",
            "Train Epoch: 2 [30000 / 60000 (50%)]\tLoss: 1.550086\n",
            "Train Epoch: 2 [32000 / 60000 (53%)]\tLoss: 1.640940\n",
            "Train Epoch: 2 [34000 / 60000 (57%)]\tLoss: 1.581751\n",
            "Train Epoch: 2 [36000 / 60000 (60%)]\tLoss: 1.552631\n",
            "Train Epoch: 2 [38000 / 60000 (63%)]\tLoss: 1.588386\n",
            "Train Epoch: 2 [40000 / 60000 (67%)]\tLoss: 1.545792\n",
            "Train Epoch: 2 [42000 / 60000 (70%)]\tLoss: 1.566364\n",
            "Train Epoch: 2 [44000 / 60000 (73%)]\tLoss: 1.565122\n",
            "Train Epoch: 2 [46000 / 60000 (77%)]\tLoss: 1.540908\n",
            "Train Epoch: 2 [48000 / 60000 (80%)]\tLoss: 1.536792\n",
            "Train Epoch: 2 [50000 / 60000 (83%)]\tLoss: 1.544070\n",
            "Train Epoch: 2 [52000 / 60000 (87%)]\tLoss: 1.554415\n",
            "Train Epoch: 2 [54000 / 60000 (90%)]\tLoss: 1.512400\n",
            "Train Epoch: 2 [56000 / 60000 (93%)]\tLoss: 1.539154\n",
            "Train Epoch: 2 [58000 / 60000 (97%)]\tLoss: 1.537490\n",
            "\n",
            "Test set -> Average loss: 0.0150 | Accuracy 9640 / 10000 (96)%\n",
            "\n",
            "Train Epoch: 3 [0 / 60000 (0%)]\tLoss: 1.502546\n",
            "Train Epoch: 3 [2000 / 60000 (3%)]\tLoss: 1.550054\n",
            "Train Epoch: 3 [4000 / 60000 (7%)]\tLoss: 1.535012\n",
            "Train Epoch: 3 [6000 / 60000 (10%)]\tLoss: 1.548603\n",
            "Train Epoch: 3 [8000 / 60000 (13%)]\tLoss: 1.518895\n",
            "Train Epoch: 3 [10000 / 60000 (17%)]\tLoss: 1.532290\n",
            "Train Epoch: 3 [12000 / 60000 (20%)]\tLoss: 1.530135\n",
            "Train Epoch: 3 [14000 / 60000 (23%)]\tLoss: 1.538067\n",
            "Train Epoch: 3 [16000 / 60000 (27%)]\tLoss: 1.553793\n",
            "Train Epoch: 3 [18000 / 60000 (30%)]\tLoss: 1.588639\n",
            "Train Epoch: 3 [20000 / 60000 (33%)]\tLoss: 1.542133\n",
            "Train Epoch: 3 [22000 / 60000 (37%)]\tLoss: 1.560076\n",
            "Train Epoch: 3 [24000 / 60000 (40%)]\tLoss: 1.541345\n",
            "Train Epoch: 3 [26000 / 60000 (43%)]\tLoss: 1.542845\n",
            "Train Epoch: 3 [28000 / 60000 (47%)]\tLoss: 1.539421\n",
            "Train Epoch: 3 [30000 / 60000 (50%)]\tLoss: 1.555323\n",
            "Train Epoch: 3 [32000 / 60000 (53%)]\tLoss: 1.496596\n",
            "Train Epoch: 3 [34000 / 60000 (57%)]\tLoss: 1.516104\n",
            "Train Epoch: 3 [36000 / 60000 (60%)]\tLoss: 1.555102\n",
            "Train Epoch: 3 [38000 / 60000 (63%)]\tLoss: 1.522928\n",
            "Train Epoch: 3 [40000 / 60000 (67%)]\tLoss: 1.546894\n",
            "Train Epoch: 3 [42000 / 60000 (70%)]\tLoss: 1.535726\n",
            "Train Epoch: 3 [44000 / 60000 (73%)]\tLoss: 1.533554\n",
            "Train Epoch: 3 [46000 / 60000 (77%)]\tLoss: 1.572004\n",
            "Train Epoch: 3 [48000 / 60000 (80%)]\tLoss: 1.527232\n",
            "Train Epoch: 3 [50000 / 60000 (83%)]\tLoss: 1.547971\n",
            "Train Epoch: 3 [52000 / 60000 (87%)]\tLoss: 1.519983\n",
            "Train Epoch: 3 [54000 / 60000 (90%)]\tLoss: 1.556927\n",
            "Train Epoch: 3 [56000 / 60000 (93%)]\tLoss: 1.520405\n",
            "Train Epoch: 3 [58000 / 60000 (97%)]\tLoss: 1.559933\n",
            "\n",
            "Test set -> Average loss: 0.0149 | Accuracy 9687 / 10000 (97)%\n",
            "\n",
            "Train Epoch: 4 [0 / 60000 (0%)]\tLoss: 1.573358\n",
            "Train Epoch: 4 [2000 / 60000 (3%)]\tLoss: 1.568708\n",
            "Train Epoch: 4 [4000 / 60000 (7%)]\tLoss: 1.499752\n",
            "Train Epoch: 4 [6000 / 60000 (10%)]\tLoss: 1.542112\n",
            "Train Epoch: 4 [8000 / 60000 (13%)]\tLoss: 1.537488\n",
            "Train Epoch: 4 [10000 / 60000 (17%)]\tLoss: 1.524790\n",
            "Train Epoch: 4 [12000 / 60000 (20%)]\tLoss: 1.552055\n",
            "Train Epoch: 4 [14000 / 60000 (23%)]\tLoss: 1.498811\n",
            "Train Epoch: 4 [16000 / 60000 (27%)]\tLoss: 1.522011\n",
            "Train Epoch: 4 [18000 / 60000 (30%)]\tLoss: 1.541498\n",
            "Train Epoch: 4 [20000 / 60000 (33%)]\tLoss: 1.552932\n",
            "Train Epoch: 4 [22000 / 60000 (37%)]\tLoss: 1.514066\n",
            "Train Epoch: 4 [24000 / 60000 (40%)]\tLoss: 1.508219\n",
            "Train Epoch: 4 [26000 / 60000 (43%)]\tLoss: 1.529493\n",
            "Train Epoch: 4 [28000 / 60000 (47%)]\tLoss: 1.496037\n",
            "Train Epoch: 4 [30000 / 60000 (50%)]\tLoss: 1.542184\n",
            "Train Epoch: 4 [32000 / 60000 (53%)]\tLoss: 1.591559\n",
            "Train Epoch: 4 [34000 / 60000 (57%)]\tLoss: 1.505068\n",
            "Train Epoch: 4 [36000 / 60000 (60%)]\tLoss: 1.532242\n",
            "Train Epoch: 4 [38000 / 60000 (63%)]\tLoss: 1.513510\n",
            "Train Epoch: 4 [40000 / 60000 (67%)]\tLoss: 1.535568\n",
            "Train Epoch: 4 [42000 / 60000 (70%)]\tLoss: 1.539248\n",
            "Train Epoch: 4 [44000 / 60000 (73%)]\tLoss: 1.525005\n",
            "Train Epoch: 4 [46000 / 60000 (77%)]\tLoss: 1.531638\n",
            "Train Epoch: 4 [48000 / 60000 (80%)]\tLoss: 1.506068\n",
            "Train Epoch: 4 [50000 / 60000 (83%)]\tLoss: 1.516110\n",
            "Train Epoch: 4 [52000 / 60000 (87%)]\tLoss: 1.510016\n",
            "Train Epoch: 4 [54000 / 60000 (90%)]\tLoss: 1.555428\n",
            "Train Epoch: 4 [56000 / 60000 (93%)]\tLoss: 1.515604\n",
            "Train Epoch: 4 [58000 / 60000 (97%)]\tLoss: 1.535436\n",
            "\n",
            "Test set -> Average loss: 0.0149 | Accuracy 9723 / 10000 (97)%\n",
            "\n",
            "Train Epoch: 5 [0 / 60000 (0%)]\tLoss: 1.500696\n",
            "Train Epoch: 5 [2000 / 60000 (3%)]\tLoss: 1.532745\n",
            "Train Epoch: 5 [4000 / 60000 (7%)]\tLoss: 1.530635\n",
            "Train Epoch: 5 [6000 / 60000 (10%)]\tLoss: 1.479787\n",
            "Train Epoch: 5 [8000 / 60000 (13%)]\tLoss: 1.500273\n",
            "Train Epoch: 5 [10000 / 60000 (17%)]\tLoss: 1.545517\n",
            "Train Epoch: 5 [12000 / 60000 (20%)]\tLoss: 1.505331\n",
            "Train Epoch: 5 [14000 / 60000 (23%)]\tLoss: 1.531937\n",
            "Train Epoch: 5 [16000 / 60000 (27%)]\tLoss: 1.556867\n",
            "Train Epoch: 5 [18000 / 60000 (30%)]\tLoss: 1.497843\n",
            "Train Epoch: 5 [20000 / 60000 (33%)]\tLoss: 1.491625\n",
            "Train Epoch: 5 [22000 / 60000 (37%)]\tLoss: 1.556092\n",
            "Train Epoch: 5 [24000 / 60000 (40%)]\tLoss: 1.511059\n",
            "Train Epoch: 5 [26000 / 60000 (43%)]\tLoss: 1.497162\n",
            "Train Epoch: 5 [28000 / 60000 (47%)]\tLoss: 1.546207\n",
            "Train Epoch: 5 [30000 / 60000 (50%)]\tLoss: 1.515293\n",
            "Train Epoch: 5 [32000 / 60000 (53%)]\tLoss: 1.531525\n",
            "Train Epoch: 5 [34000 / 60000 (57%)]\tLoss: 1.517935\n",
            "Train Epoch: 5 [36000 / 60000 (60%)]\tLoss: 1.558226\n",
            "Train Epoch: 5 [38000 / 60000 (63%)]\tLoss: 1.587468\n",
            "Train Epoch: 5 [40000 / 60000 (67%)]\tLoss: 1.553491\n",
            "Train Epoch: 5 [42000 / 60000 (70%)]\tLoss: 1.526265\n",
            "Train Epoch: 5 [44000 / 60000 (73%)]\tLoss: 1.502807\n",
            "Train Epoch: 5 [46000 / 60000 (77%)]\tLoss: 1.526015\n",
            "Train Epoch: 5 [48000 / 60000 (80%)]\tLoss: 1.516052\n",
            "Train Epoch: 5 [50000 / 60000 (83%)]\tLoss: 1.554991\n",
            "Train Epoch: 5 [52000 / 60000 (87%)]\tLoss: 1.543682\n",
            "Train Epoch: 5 [54000 / 60000 (90%)]\tLoss: 1.533054\n",
            "Train Epoch: 5 [56000 / 60000 (93%)]\tLoss: 1.558751\n",
            "Train Epoch: 5 [58000 / 60000 (97%)]\tLoss: 1.535484\n",
            "\n",
            "Test set -> Average loss: 0.0149 | Accuracy 9733 / 10000 (97)%\n",
            "\n",
            "Train Epoch: 6 [0 / 60000 (0%)]\tLoss: 1.549516\n",
            "Train Epoch: 6 [2000 / 60000 (3%)]\tLoss: 1.505105\n",
            "Train Epoch: 6 [4000 / 60000 (7%)]\tLoss: 1.576999\n",
            "Train Epoch: 6 [6000 / 60000 (10%)]\tLoss: 1.509252\n",
            "Train Epoch: 6 [8000 / 60000 (13%)]\tLoss: 1.589522\n",
            "Train Epoch: 6 [10000 / 60000 (17%)]\tLoss: 1.529948\n",
            "Train Epoch: 6 [12000 / 60000 (20%)]\tLoss: 1.529578\n",
            "Train Epoch: 6 [14000 / 60000 (23%)]\tLoss: 1.506748\n",
            "Train Epoch: 6 [16000 / 60000 (27%)]\tLoss: 1.542020\n",
            "Train Epoch: 6 [18000 / 60000 (30%)]\tLoss: 1.571334\n",
            "Train Epoch: 6 [20000 / 60000 (33%)]\tLoss: 1.523208\n",
            "Train Epoch: 6 [22000 / 60000 (37%)]\tLoss: 1.549706\n",
            "Train Epoch: 6 [24000 / 60000 (40%)]\tLoss: 1.508815\n",
            "Train Epoch: 6 [26000 / 60000 (43%)]\tLoss: 1.537106\n",
            "Train Epoch: 6 [28000 / 60000 (47%)]\tLoss: 1.484501\n",
            "Train Epoch: 6 [30000 / 60000 (50%)]\tLoss: 1.527387\n",
            "Train Epoch: 6 [32000 / 60000 (53%)]\tLoss: 1.540738\n",
            "Train Epoch: 6 [34000 / 60000 (57%)]\tLoss: 1.568284\n",
            "Train Epoch: 6 [36000 / 60000 (60%)]\tLoss: 1.582934\n",
            "Train Epoch: 6 [38000 / 60000 (63%)]\tLoss: 1.498187\n",
            "Train Epoch: 6 [40000 / 60000 (67%)]\tLoss: 1.543229\n",
            "Train Epoch: 6 [42000 / 60000 (70%)]\tLoss: 1.507828\n",
            "Train Epoch: 6 [44000 / 60000 (73%)]\tLoss: 1.532714\n",
            "Train Epoch: 6 [46000 / 60000 (77%)]\tLoss: 1.512556\n",
            "Train Epoch: 6 [48000 / 60000 (80%)]\tLoss: 1.531929\n",
            "Train Epoch: 6 [50000 / 60000 (83%)]\tLoss: 1.528171\n",
            "Train Epoch: 6 [52000 / 60000 (87%)]\tLoss: 1.506486\n",
            "Train Epoch: 6 [54000 / 60000 (90%)]\tLoss: 1.583981\n",
            "Train Epoch: 6 [56000 / 60000 (93%)]\tLoss: 1.497576\n",
            "Train Epoch: 6 [58000 / 60000 (97%)]\tLoss: 1.539912\n",
            "\n",
            "Test set -> Average loss: 0.0149 | Accuracy 9751 / 10000 (98)%\n",
            "\n",
            "Train Epoch: 7 [0 / 60000 (0%)]\tLoss: 1.474037\n",
            "Train Epoch: 7 [2000 / 60000 (3%)]\tLoss: 1.530564\n",
            "Train Epoch: 7 [4000 / 60000 (7%)]\tLoss: 1.494410\n",
            "Train Epoch: 7 [6000 / 60000 (10%)]\tLoss: 1.502961\n",
            "Train Epoch: 7 [8000 / 60000 (13%)]\tLoss: 1.519324\n",
            "Train Epoch: 7 [10000 / 60000 (17%)]\tLoss: 1.517801\n",
            "Train Epoch: 7 [12000 / 60000 (20%)]\tLoss: 1.530164\n",
            "Train Epoch: 7 [14000 / 60000 (23%)]\tLoss: 1.552087\n",
            "Train Epoch: 7 [16000 / 60000 (27%)]\tLoss: 1.480286\n",
            "Train Epoch: 7 [18000 / 60000 (30%)]\tLoss: 1.537100\n",
            "Train Epoch: 7 [20000 / 60000 (33%)]\tLoss: 1.566046\n",
            "Train Epoch: 7 [22000 / 60000 (37%)]\tLoss: 1.549364\n",
            "Train Epoch: 7 [24000 / 60000 (40%)]\tLoss: 1.513792\n",
            "Train Epoch: 7 [26000 / 60000 (43%)]\tLoss: 1.535000\n",
            "Train Epoch: 7 [28000 / 60000 (47%)]\tLoss: 1.549061\n",
            "Train Epoch: 7 [30000 / 60000 (50%)]\tLoss: 1.532334\n",
            "Train Epoch: 7 [32000 / 60000 (53%)]\tLoss: 1.525410\n",
            "Train Epoch: 7 [34000 / 60000 (57%)]\tLoss: 1.536178\n",
            "Train Epoch: 7 [36000 / 60000 (60%)]\tLoss: 1.542010\n",
            "Train Epoch: 7 [38000 / 60000 (63%)]\tLoss: 1.522370\n",
            "Train Epoch: 7 [40000 / 60000 (67%)]\tLoss: 1.510401\n",
            "Train Epoch: 7 [42000 / 60000 (70%)]\tLoss: 1.510879\n",
            "Train Epoch: 7 [44000 / 60000 (73%)]\tLoss: 1.520495\n",
            "Train Epoch: 7 [46000 / 60000 (77%)]\tLoss: 1.548628\n",
            "Train Epoch: 7 [48000 / 60000 (80%)]\tLoss: 1.512723\n",
            "Train Epoch: 7 [50000 / 60000 (83%)]\tLoss: 1.500592\n",
            "Train Epoch: 7 [52000 / 60000 (87%)]\tLoss: 1.501269\n",
            "Train Epoch: 7 [54000 / 60000 (90%)]\tLoss: 1.516409\n",
            "Train Epoch: 7 [56000 / 60000 (93%)]\tLoss: 1.520642\n",
            "Train Epoch: 7 [58000 / 60000 (97%)]\tLoss: 1.508862\n",
            "\n",
            "Test set -> Average loss: 0.0149 | Accuracy 9765 / 10000 (98)%\n",
            "\n",
            "Train Epoch: 8 [0 / 60000 (0%)]\tLoss: 1.512599\n",
            "Train Epoch: 8 [2000 / 60000 (3%)]\tLoss: 1.549773\n",
            "Train Epoch: 8 [4000 / 60000 (7%)]\tLoss: 1.491520\n",
            "Train Epoch: 8 [6000 / 60000 (10%)]\tLoss: 1.550079\n",
            "Train Epoch: 8 [8000 / 60000 (13%)]\tLoss: 1.512456\n",
            "Train Epoch: 8 [10000 / 60000 (17%)]\tLoss: 1.525583\n",
            "Train Epoch: 8 [12000 / 60000 (20%)]\tLoss: 1.522907\n",
            "Train Epoch: 8 [14000 / 60000 (23%)]\tLoss: 1.526502\n",
            "Train Epoch: 8 [16000 / 60000 (27%)]\tLoss: 1.512904\n",
            "Train Epoch: 8 [18000 / 60000 (30%)]\tLoss: 1.568671\n",
            "Train Epoch: 8 [20000 / 60000 (33%)]\tLoss: 1.524645\n",
            "Train Epoch: 8 [22000 / 60000 (37%)]\tLoss: 1.543121\n",
            "Train Epoch: 8 [24000 / 60000 (40%)]\tLoss: 1.503682\n",
            "Train Epoch: 8 [26000 / 60000 (43%)]\tLoss: 1.544533\n",
            "Train Epoch: 8 [28000 / 60000 (47%)]\tLoss: 1.518524\n",
            "Train Epoch: 8 [30000 / 60000 (50%)]\tLoss: 1.508326\n",
            "Train Epoch: 8 [32000 / 60000 (53%)]\tLoss: 1.507450\n",
            "Train Epoch: 8 [34000 / 60000 (57%)]\tLoss: 1.521869\n",
            "Train Epoch: 8 [36000 / 60000 (60%)]\tLoss: 1.510031\n",
            "Train Epoch: 8 [38000 / 60000 (63%)]\tLoss: 1.499224\n",
            "Train Epoch: 8 [40000 / 60000 (67%)]\tLoss: 1.518125\n",
            "Train Epoch: 8 [42000 / 60000 (70%)]\tLoss: 1.522080\n",
            "Train Epoch: 8 [44000 / 60000 (73%)]\tLoss: 1.533804\n",
            "Train Epoch: 8 [46000 / 60000 (77%)]\tLoss: 1.503052\n",
            "Train Epoch: 8 [48000 / 60000 (80%)]\tLoss: 1.506461\n",
            "Train Epoch: 8 [50000 / 60000 (83%)]\tLoss: 1.516810\n",
            "Train Epoch: 8 [52000 / 60000 (87%)]\tLoss: 1.526690\n",
            "Train Epoch: 8 [54000 / 60000 (90%)]\tLoss: 1.528228\n",
            "Train Epoch: 8 [56000 / 60000 (93%)]\tLoss: 1.500341\n",
            "Train Epoch: 8 [58000 / 60000 (97%)]\tLoss: 1.483941\n",
            "\n",
            "Test set -> Average loss: 0.0148 | Accuracy 9778 / 10000 (98)%\n",
            "\n",
            "Train Epoch: 9 [0 / 60000 (0%)]\tLoss: 1.508941\n",
            "Train Epoch: 9 [2000 / 60000 (3%)]\tLoss: 1.543112\n",
            "Train Epoch: 9 [4000 / 60000 (7%)]\tLoss: 1.478841\n",
            "Train Epoch: 9 [6000 / 60000 (10%)]\tLoss: 1.520919\n",
            "Train Epoch: 9 [8000 / 60000 (13%)]\tLoss: 1.505715\n",
            "Train Epoch: 9 [10000 / 60000 (17%)]\tLoss: 1.551866\n",
            "Train Epoch: 9 [12000 / 60000 (20%)]\tLoss: 1.513996\n",
            "Train Epoch: 9 [14000 / 60000 (23%)]\tLoss: 1.479329\n",
            "Train Epoch: 9 [16000 / 60000 (27%)]\tLoss: 1.526601\n",
            "Train Epoch: 9 [18000 / 60000 (30%)]\tLoss: 1.520328\n",
            "Train Epoch: 9 [20000 / 60000 (33%)]\tLoss: 1.502536\n",
            "Train Epoch: 9 [22000 / 60000 (37%)]\tLoss: 1.517404\n",
            "Train Epoch: 9 [24000 / 60000 (40%)]\tLoss: 1.516975\n",
            "Train Epoch: 9 [26000 / 60000 (43%)]\tLoss: 1.546353\n",
            "Train Epoch: 9 [28000 / 60000 (47%)]\tLoss: 1.575193\n",
            "Train Epoch: 9 [30000 / 60000 (50%)]\tLoss: 1.502794\n",
            "Train Epoch: 9 [32000 / 60000 (53%)]\tLoss: 1.537506\n",
            "Train Epoch: 9 [34000 / 60000 (57%)]\tLoss: 1.493353\n",
            "Train Epoch: 9 [36000 / 60000 (60%)]\tLoss: 1.502726\n",
            "Train Epoch: 9 [38000 / 60000 (63%)]\tLoss: 1.501562\n",
            "Train Epoch: 9 [40000 / 60000 (67%)]\tLoss: 1.529891\n",
            "Train Epoch: 9 [42000 / 60000 (70%)]\tLoss: 1.516834\n",
            "Train Epoch: 9 [44000 / 60000 (73%)]\tLoss: 1.531752\n",
            "Train Epoch: 9 [46000 / 60000 (77%)]\tLoss: 1.489101\n",
            "Train Epoch: 9 [48000 / 60000 (80%)]\tLoss: 1.549215\n",
            "Train Epoch: 9 [50000 / 60000 (83%)]\tLoss: 1.518585\n",
            "Train Epoch: 9 [52000 / 60000 (87%)]\tLoss: 1.492771\n",
            "Train Epoch: 9 [54000 / 60000 (90%)]\tLoss: 1.551646\n",
            "Train Epoch: 9 [56000 / 60000 (93%)]\tLoss: 1.538915\n",
            "Train Epoch: 9 [58000 / 60000 (97%)]\tLoss: 1.558836\n",
            "\n",
            "Test set -> Average loss: 0.0148 | Accuracy 9782 / 10000 (98)%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "\n",
        "data, target = test_data[80]\n",
        "\n",
        "print(data.size())\n",
        "print(\"\\n\")\n",
        "\n",
        "data = data.unsqueeze(0).to(device)\n",
        "\n",
        "print(data.size())\n",
        "print(\"\\n\")\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "prediction = output.argmax(dim = 1, keepdim = True).item()\n",
        "\n",
        "print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# Unpack the image from the data, squeeze out channel dimension\n",
        "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "plt.imshow(image, cmap = \"gray\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "5rqIVtRJQEoX",
        "outputId": "a26a57fe-dd1e-4979-9bd6-f401d76522fa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "\n",
            "\n",
            "torch.Size([1, 1, 28, 28])\n",
            "\n",
            "\n",
            "Prediction: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-57077a1fc1fa>:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.softmax(x) # Returns probability of each number\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image_path = \"2.jpeg\"\n",
        "custom_image_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(28, 28))\n",
        "])"
      ],
      "metadata": {
        "id": "ykIPFfW5TWBw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "d12d81bc-ca7a-40e3-f920-92853c293e74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transforms' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a49666d710a7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcustom_image_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.jpeg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m custom_image_transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
        "\n",
        "custom_image = custom_image / 255\n",
        "\n",
        "# Convert to grayscale by averaging the color channels (assuming a color image)\n",
        "if custom_image.shape[0] == 3:\n",
        "    custom_image = custom_image.mean(dim=0, keepdim=True)\n",
        "\n",
        "custom_image = custom_image_transform(custom_image)\n",
        "\n",
        "print(custom_image.size())\n",
        "print(\"\\n\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  custom_image = custom_image.unsqueeze(0)\n",
        "\n",
        "  output = model(custom_image.to(device))\n",
        "\n",
        "  prediction = output.argmax(dim = 1, keepdim = True).item()\n",
        "\n",
        "  print(f\"Prediction: {prediction}\")\n",
        "\n",
        "  # Unpack the image from the data, squeeze out channel dimension\n",
        "  image = custom_image.squeeze(0).squeeze(0).cpu().numpy()\n",
        "\n",
        "  plt.imshow(image)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "qbE62c6fT0A9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}